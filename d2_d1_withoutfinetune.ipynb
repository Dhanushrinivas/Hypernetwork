{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0777a2-191b-4a8e-a631-f8dd7f06c87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhanushsrinivas K\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Hypernetwork on d2 ---\n",
      "Epoch 1, Hypernetwork Training Loss on d2: 0.6896\n",
      "Epoch 2, Hypernetwork Training Loss on d2: 0.6740\n",
      "Epoch 3, Hypernetwork Training Loss on d2: 0.6532\n",
      "Epoch 4, Hypernetwork Training Loss on d2: 0.6231\n",
      "Epoch 5, Hypernetwork Training Loss on d2: 0.5803\n",
      "Epoch 6, Hypernetwork Training Loss on d2: 0.5228\n",
      "Epoch 7, Hypernetwork Training Loss on d2: 0.4516\n",
      "Epoch 8, Hypernetwork Training Loss on d2: 0.3746\n",
      "Epoch 9, Hypernetwork Training Loss on d2: 0.3069\n",
      "Epoch 10, Hypernetwork Training Loss on d2: 0.2673\n",
      "\n",
      "--- Evaluating on d1 using weights from Hypernetwork ---\n",
      "\n",
      "FINAL ACCURACY on d1 test set (Direct Transfer): 0.6800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "d1_path = r\"E:\\Copy of default_of_credit_card_clients(1).csv\"\n",
    "d2_path = r\"E:\\LOAN.csv\"\n",
    "\n",
    "d1 = pd.read_csv(d1_path, header=1)\n",
    "d1.rename(columns={'default payment next month': 'target'}, inplace=True)\n",
    "d1 = d1.drop(columns=['ID'], errors='ignore')\n",
    "\n",
    "d2 = pd.read_csv(d2_path)\n",
    "d2.rename(columns={'y': 'target'}, inplace=True)\n",
    "\n",
    "# Feature selection\n",
    "d1_num_features = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'PAY_AMT1']\n",
    "d2_num_features = ['duration', 'age', 'campaign', 'pdays']\n",
    "d1_cat_features = ['MARRIAGE']\n",
    "d2_cat_features = ['marital']\n",
    "\n",
    "overlapping_num_features = d1_num_features\n",
    "overlapping_cat_features = ['MARRIAGE']\n",
    "\n",
    "num_rename_map = {d2_col: d1_col for d2_col, d1_col in zip(d2_num_features, d1_num_features)}\n",
    "cat_rename_map = {d2_col: d1_col for d2_col, d1_col in zip(d2_cat_features, d1_cat_features)}\n",
    "d2_renamed = d2.rename(columns={**num_rename_map, **cat_rename_map})\n",
    "\n",
    "all_features = overlapping_num_features + overlapping_cat_features\n",
    "X1 = d1[all_features].copy()\n",
    "X2 = d2_renamed[all_features].copy()\n",
    "y1 = d1['target'].astype(int)\n",
    "y2 = d2_renamed['target'].apply(lambda x: 1 if x == 'yes' else 0).astype(int)\n",
    "\n",
    "# Convert categorical columns to string\n",
    "for col in overlapping_cat_features:\n",
    "    X1[col] = X1[col].astype(str)\n",
    "    X2[col] = X2[col].astype(str)\n",
    "\n",
    "# Clean numerical columns\n",
    "for col in overlapping_num_features:\n",
    "    X1[col] = pd.to_numeric(X1[col], errors='coerce')\n",
    "    X1[col] = X1[col].fillna(X1[col].median())\n",
    "    \n",
    "    X2[col] = pd.to_numeric(X2[col], errors='coerce')\n",
    "    X2[col] = X2[col].fillna(X2[col].median())\n",
    "\n",
    "# Preprocessor is FIT on d2 (hypernetwork data)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), overlapping_num_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), overlapping_cat_features)\n",
    "    ])\n",
    "\n",
    "X2_processed = preprocessor.fit_transform(X2)\n",
    "X1_processed = preprocessor.transform(X1) \n",
    "\n",
    "# Split data\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1_processed, y1, test_size=0.2, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_processed, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. Model Definition ---\n",
    "INPUT_SHAPE = [X2_train.shape[1]]\n",
    "\n",
    "def create_main_network():\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(64, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name=\"main_network\")\n",
    "    return model\n",
    "\n",
    "main_network_template = create_main_network()\n",
    "total_params = main_network_template.count_params()\n",
    "param_shapes = [w.shape for w in main_network_template.get_weights()]\n",
    "param_sizes = [tf.size(w).numpy() for w in main_network_template.get_weights()]\n",
    "\n",
    "def create_hypernetwork(total_params):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(128, activation='relu', input_shape=INPUT_SHAPE),\n",
    "        Dense(total_params, activation=None)\n",
    "    ], name=\"hypernetwork\")\n",
    "    return model\n",
    "\n",
    "hypernetwork = create_hypernetwork(total_params)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# --- 3. Hypernetwork Training on d2 ---\n",
    "@tf.function\n",
    "def train_step(X_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        generated_weights_flat = hypernetwork(X_batch)\n",
    "        avg_weights_flat = tf.reduce_mean(generated_weights_flat, axis=0)\n",
    "        \n",
    "        current_pos = 0\n",
    "        w1 = tf.reshape(avg_weights_flat[current_pos : current_pos+param_sizes[0]], param_shapes[0]); current_pos += param_sizes[0]\n",
    "        b1 = tf.reshape(avg_weights_flat[current_pos : current_pos+param_sizes[1]], param_shapes[1]); current_pos += param_sizes[1]\n",
    "        layer1_out = tf.nn.relu(tf.matmul(X_batch, w1) + b1)\n",
    "        \n",
    "        w2 = tf.reshape(avg_weights_flat[current_pos : current_pos+param_sizes[2]], param_shapes[2]); current_pos += param_sizes[2]\n",
    "        b2 = tf.reshape(avg_weights_flat[current_pos : current_pos+param_sizes[3]], param_shapes[3]); current_pos += param_sizes[3]\n",
    "        layer2_out = tf.nn.relu(tf.matmul(layer1_out, w2) + b2)\n",
    "\n",
    "        w3 = tf.reshape(avg_weights_flat[current_pos : current_pos+param_sizes[4]], param_shapes[4]); current_pos += param_sizes[4]\n",
    "        b3 = tf.reshape(avg_weights_flat[current_pos : current_pos+param_sizes[5]], param_shapes[5])\n",
    "        predictions = tf.nn.sigmoid(tf.matmul(layer2_out, w3) + b3)\n",
    "        \n",
    "        loss = loss_fn(y_batch, predictions)\n",
    "\n",
    "    grads = tape.gradient(loss, hypernetwork.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, hypernetwork.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "print(\"--- Training Hypernetwork on d2 ---\")\n",
    "for epoch in range(10):\n",
    "    loss = train_step(tf.constant(X2_train, dtype=tf.float32), tf.constant(y2_train.values, dtype=tf.float32)[:, None])\n",
    "    print(f\"Epoch {epoch+1}, Hypernetwork Training Loss on d2: {loss.numpy():.4f}\")\n",
    "\n",
    "# --- 4. Evaluation on d1 (Without Fine-Tuning) ---\n",
    "print(\"\\n--- Evaluating on d1 using weights from Hypernetwork ---\")\n",
    "final_weights_flat = tf.reduce_mean(hypernetwork(tf.constant(X1_test, dtype=tf.float32)), axis=0)\n",
    "\n",
    "final_weights_structured = []\n",
    "current_pos = 0\n",
    "for shape, size in zip(param_shapes, param_sizes):\n",
    "    param = tf.reshape(final_weights_flat[current_pos : current_pos+size], shape)\n",
    "    final_weights_structured.append(param)\n",
    "    current_pos += size\n",
    "\n",
    "main_network_template.set_weights(final_weights_structured)\n",
    "main_network_template.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Evaluate on d1 test set\n",
    "d1_accuracy = main_network_template.evaluate(X1_test, y1_test, verbose=0)[1]\n",
    "print(f\"\\nFINAL ACCURACY on d1 test set (Direct Transfer): {d1_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd26b8-ba4c-4375-bfa6-c9becac1f70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
